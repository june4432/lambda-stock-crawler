"""
ì£¼ì‹ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ - AWS Lambdaìš© ë©”ì¸ ì§„ì…ì 
íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” íŒ©í† ë¦¬ ë©”ì„œë“œ
"""

import json
import os
import asyncio
from datetime import datetime
import boto3
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ)
if os.path.exists('.env'):
    load_dotenv()


def factory_lambda_handler(event, context):
    """
    ì£¼ì‹ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ Lambda í•¸ë“¤ëŸ¬
    
    Args:
        event (dict): Lambda ì´ë²¤íŠ¸ ë°ì´í„°
            - crawler_type: 'daily', 'quarter', 'annual'
            - s3_bucket: S3 ë²„í‚·ëª… (ì„ íƒì‚¬í•­)
            - delay_between_stocks: ì¢…ëª© ê°„ ëŒ€ê¸°ì‹œê°„ (ì„ íƒì‚¬í•­)
        context: Lambda ì»¨í…ìŠ¤íŠ¸
        
    Returns:
        dict: HTTP ì‘ë‹µ
    """
    try:
        # í™˜ê²½ë³€ìˆ˜ ìš°ì„ ìˆœìœ„ë¡œ ì„¤ì •ê°’ ì½ê¸° (í™˜ê²½ë³€ìˆ˜ > ì´ë²¤íŠ¸ íŒŒë¼ë¯¸í„° > ê¸°ë³¸ê°’)
        crawler_type = os.environ.get('CRAWLER_TYPE') or event.get('crawler_type') or 'daily'
        s3_bucket = os.environ.get('S3_BUCKET') or event.get('s3_bucket') or 'test-stock-info-bucket'
        delay_between_stocks = int(os.environ.get('DELAY_BETWEEN_STOCKS') or event.get('delay_between_stocks') or '2')
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ê°’ ì¶œë ¥
        print(f"ğŸ” í™˜ê²½ë³€ìˆ˜ ë””ë²„ê¹…:")
        print(f"   CRAWLER_TYPE: {os.environ.get('CRAWLER_TYPE', 'None')}")
        print(f"   S3_BUCKET: {os.environ.get('S3_BUCKET', 'None')}")
        print(f"   DELAY_BETWEEN_STOCKS: {os.environ.get('DELAY_BETWEEN_STOCKS', 'None')}")
        print(f"ğŸ” ì´ë²¤íŠ¸ íŒŒë¼ë¯¸í„°:")
        print(f"   crawler_type: {event.get('crawler_type', 'None')}")
        print(f"   s3_bucket: {event.get('s3_bucket', 'None')}")
        print(f"   delay_between_stocks: {event.get('delay_between_stocks', 'None')}")
        print(f"ğŸš€ ì£¼ì‹ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ ì‹œì‘ - íƒ€ì…: {crawler_type}, ë²„í‚·: {s3_bucket}, ë”œë ˆì´: {delay_between_stocks}ì´ˆ")
        
        # ì´ë²¤íŠ¸ì— í™˜ê²½ë³€ìˆ˜ ê°’ë“¤ ì¶”ê°€
        event_with_env = event.copy()
        event_with_env['s3_bucket'] = s3_bucket
        event_with_env['delay_between_stocks'] = delay_between_stocks
        
        # íƒ€ì…ì— ë”°ë¼ ë¶„ê¸°
        if crawler_type == 'daily':
            return handle_daily_crawler(event_with_env, context)
        elif crawler_type == 'quarter':
            return handle_quarter_crawler(event_with_env, context)
        elif crawler_type == 'annual':
            return handle_annual_crawler(event_with_env, context)
        else:
            return {
                'statusCode': 400,
                'headers': {
                    'Content-Type': 'application/json; charset=utf-8'
                },
                'body': json.dumps({
                    'success': False,
                    'error': f'ì§€ì›í•˜ì§€ ì•ŠëŠ” í¬ë¡¤ëŸ¬ íƒ€ì…: {crawler_type}',
                    'supported_types': ['daily', 'quarter', 'annual']
                }, ensure_ascii=False, indent=2)
            }
            
    except Exception as e:
        return {
            'statusCode': 500,
            'headers': {
                'Content-Type': 'application/json; charset=utf-8'
            },
            'body': json.dumps({
                'success': False,
                'error': f'íŒ©í† ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }, ensure_ascii=False, indent=2)
        }


def handle_daily_crawler(event, context):
    """
    ì¼ê°„ íˆ¬ìì •ë³´ í¬ë¡¤ëŸ¬ ì‹¤í–‰ (PER/EPS)
    """
    print("ğŸ“Š ì¼ê°„ íˆ¬ìì •ë³´(PER/EPS) í¬ë¡¤ëŸ¬ ì‹¤í–‰")
    
    # ê¸°ì¡´ naver_stock_invest_info_crawler ëª¨ë“ˆ import
    from naver_stock_invest_info_crawler import lambda_handler
    
    # í•´ë‹¹ í•¸ë“¤ëŸ¬ ì‹¤í–‰
    return lambda_handler(event, context)


def handle_quarter_crawler(event, context):
    """
    ë¶„ê¸°ë³„ ì¬ë¬´ì •ë³´ í¬ë¡¤ëŸ¬ ì‹¤í–‰
    """
    print("ğŸ“ˆ ë¶„ê¸°ë³„ ì¬ë¬´ì •ë³´ í¬ë¡¤ëŸ¬ ì‹¤í–‰")
    
    try:
        # naver_stock_invest_index_crawler ëª¨ë“ˆ import
        from naver_stock_invest_index_crawler import run_multiple_crawler
        
        # stocks.json íŒŒì¼ ë¡œë“œ
        try:
            with open('stocks.json', 'r', encoding='utf-8') as f:
                stocks = json.load(f)
            print(f"ğŸ“‹ stocks.jsonì—ì„œ {len(stocks)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json; charset=utf-8'
                },
                'body': json.dumps({
                    'success': False,
                    'error': f'stocks.json íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}'
                }, ensure_ascii=False, indent=2)
            }
        
        # ë¶„ê¸°ë³„ í¬ë¡¤ë§ ì‹¤í–‰
        print("ğŸš€ ë¶„ê¸°ë³„ ì¬ë¬´ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        
        # ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = "/tmp/crawl_results"
        os.makedirs(output_dir, exist_ok=True)
        
        # run_multiple_crawler ì‹¤í–‰ (ë¶„ê¸°) - S3 ì—…ë¡œë“œëŠ” ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨
        s3_bucket = os.environ.get('S3_BUCKET') or event.get('s3_bucket') or 'test-stock-info-bucket'
        result = run_multiple_crawler("stocks.json", output_dir, "ë¶„ê¸°", s3_bucket)
        
        # S3 ì—…ë¡œë“œ ê²°ê³¼ëŠ” run_multiple_crawler ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬ ë¶ˆí•„ìš”
        s3_upload_result = {
            "success": True,
            "message": "S3 ì—…ë¡œë“œëŠ” run_multiple_crawler ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨"
        }
        
        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json; charset=utf-8'
            },
            'body': json.dumps({
                'success': True,
                'crawler_type': 'quarter',
                'crawl_result': result,
                's3_upload': s3_upload_result,
                'crawl_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }, ensure_ascii=False, indent=2)
        }
        
    except Exception as e:
        return {
            'statusCode': 500,
            'headers': {
                'Content-Type': 'application/json; charset=utf-8'
            },
            'body': json.dumps({
                'success': False,
                'error': f'ë¶„ê¸°ë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }, ensure_ascii=False, indent=2)
        }


def handle_annual_crawler(event, context):
    """
    ì—°ê°„ ì¬ë¬´ì •ë³´ í¬ë¡¤ëŸ¬ ì‹¤í–‰
    """
    print("ğŸ“Š ì—°ê°„ ì¬ë¬´ì •ë³´ í¬ë¡¤ëŸ¬ ì‹¤í–‰")
    
    try:
        # naver_stock_invest_index_crawler ëª¨ë“ˆ import
        from naver_stock_invest_index_crawler import run_multiple_crawler
        
        # stocks.json íŒŒì¼ ë¡œë“œ
        try:
            with open('stocks.json', 'r', encoding='utf-8') as f:
                stocks = json.load(f)
            print(f"ğŸ“‹ stocks.jsonì—ì„œ {len(stocks)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json; charset=utf-8'
                },
                'body': json.dumps({
                    'success': False,
                    'error': f'stocks.json íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}'
                }, ensure_ascii=False, indent=2)
            }
        
        # ì—°ê°„ í¬ë¡¤ë§ ì‹¤í–‰
        print("ğŸš€ ì—°ê°„ ì¬ë¬´ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        
        # ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = "/tmp/crawl_results"
        os.makedirs(output_dir, exist_ok=True)
        
        # run_multiple_crawler ì‹¤í–‰ (ì—°ê°„) - S3 ì—…ë¡œë“œëŠ” ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨
        s3_bucket = os.environ.get('S3_BUCKET') or event.get('s3_bucket') or 'test-stock-info-bucket'
        result = run_multiple_crawler("stocks.json", output_dir, "ì—°ê°„", s3_bucket)
        
        # S3 ì—…ë¡œë“œ ê²°ê³¼ëŠ” run_multiple_crawler ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬ ë¶ˆí•„ìš”
        s3_upload_result = {
            "success": True,
            "message": "S3 ì—…ë¡œë“œëŠ” run_multiple_crawler ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë¨"
        }
        
        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json; charset=utf-8'
            },
            'body': json.dumps({
                'success': True,
                'crawler_type': 'annual',
                'crawl_result': result,
                's3_upload': s3_upload_result,
                'crawl_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }, ensure_ascii=False, indent=2)
        }
        
    except Exception as e:
        return {
            'statusCode': 500,
            'headers': {
                'Content-Type': 'application/json; charset=utf-8'
            },
            'body': json.dumps({
                'success': False,
                'error': f'ì—°ê°„ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }, ensure_ascii=False, indent=2)
        }


if __name__ == "__main__":
    print("ğŸ­ ì£¼ì‹ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ - ë¡œì»¬ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì´ë²¤íŠ¸ (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
    test_events = [
        # {
        #     'crawler_type': os.environ.get('CRAWLER_TYPE', 'daily'),
        #     's3_bucket': os.environ.get('S3_BUCKET', 'test-stock-info-bucket'),
        #     'delay_between_stocks': int(os.environ.get('DELAY_BETWEEN_STOCKS', '2'))
        # },
        {
           'crawler_type': 'quarter',
           's3_bucket': os.environ.get('S3_BUCKET', 'test-stock-info-bucket'),
           'delay_between_stocks': int(os.environ.get('DELAY_BETWEEN_STOCKS', '2'))
        }
        # ,
        # {
        #    'crawler_type': 'annual',
        #    's3_bucket': os.environ.get('S3_BUCKET', 'test-stock-info-bucket'),
        #    'delay_between_stocks': int(os.environ.get('DELAY_BETWEEN_STOCKS', '2'))
        # }
    ]
    
    for test_event in test_events:
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸: {test_event['crawler_type']}")
        result = factory_lambda_handler(test_event, None)
        print(f"ê²°ê³¼ ìƒíƒœì½”ë“œ: {result['statusCode']}")
        
        if result['statusCode'] == 200:
            body = json.loads(result['body'])
            print(f"âœ… ì„±ê³µ: {body.get('success', False)}")
        else:
            print(f"âŒ ì‹¤íŒ¨: {result['statusCode']}")
