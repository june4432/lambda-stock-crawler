"""
ë„¤ì´ë²„ ê¸ˆìœµ PER/EPS ì •ë³´ í¬ë¡¤ëŸ¬ - AWS Lambdaìš©
ë„¤ì´ë²„ ê¸ˆìœµ ì¢…ëª©ì •ë³´ í˜ì´ì§€ì—ì„œ íˆ¬ìì •ë³´ íƒ­ì˜ PER/EPS ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
Lambda í™˜ê²½ì—ì„œ ì‹¤í–‰ë˜ë©° ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
"""

import asyncio
import json
from datetime import datetime
import re
from playwright.async_api import async_playwright
import traceback
import csv
import io
import os
import sys
from s3_utils import upload_csv_content_to_s3, generate_s3_key
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ (ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ)
if os.path.exists('.env'):
    load_dotenv()


class NaverFinancePERCrawlerForLambda:
    def __init__(self, headless=None, wait_timeout=None):
        """
        ë„¤ì´ë²„ ê¸ˆìœµ PER/EPS í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” (Lambdaìš©)
        
        Args:
            headless (bool): ë¸Œë¼ìš°ì €ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í• ì§€ ì—¬ë¶€ (í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
            wait_timeout (int): ìš”ì†Œ ëŒ€ê¸° ì‹œê°„ (ë°€ë¦¬ì´ˆ) (í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
        """
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì •ê°’ ì½ê¸°
        self.headless = headless if headless is not None else os.environ.get('HEADLESS', 'true').lower() == 'true'
        self.wait_timeout = wait_timeout or int(os.environ.get('WAIT_TIMEOUT', '15000'))
        self.browser = None
        self.page = None
        self.start_time = None
        self.end_time = None

        # --- ì—¬ê¸°ì— ë¸Œë¼ìš°ì € ì¸ìˆ˜ ì •ì˜ ì¶”ê°€ ---
        # Lambda í™˜ê²½ ê°ì§€
        is_lambda = os.environ.get('AWS_LAMBDA_FUNCTION_NAME') is not None
        
        self.browser_args = [
            '--no-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',
            '--disable-web-security',
            '--disable-blink-features=AutomationControlled',
            '--disable-setuid-sandbox',     # SUID ìƒŒë“œë°•ìŠ¤ ë¹„í™œì„±í™”
            '--disable-infobars',           # ì •ë³´ í‘œì‹œì¤„ ë¹„í™œì„±í™”
            '--disable-extensions',         # í™•ì¥ ê¸°ëŠ¥ ë¹„í™œì„±í™”
            '--disable-background-networking', # ë°±ê·¸ë¼ìš´ë“œ ë„¤íŠ¸ì›Œí¬ ì‘ì—… ë¹„í™œì„±í™”
            '--disable-component-extensions-with-background-pages',
        ]
        
        # Lambda í™˜ê²½ì—ì„œë§Œ ì¶”ê°€ ì˜µì…˜ ì‚¬ìš©
        if is_lambda:
            self.browser_args.extend([
                '--single-process',         # Lambdaì—ì„œë§Œ ì‚¬ìš©
                '--no-zygote',              # Lambdaì—ì„œë§Œ ì‚¬ìš©
            ])
        # --- ì—¬ê¸°ê¹Œì§€ ---
    
    def start_timer(self):
        """í¬ë¡¤ë§ ì‹œì‘ ì‹œê°„ ê¸°ë¡"""
        self.start_time = datetime.now()
        timestamp = self.start_time.strftime("%Y-%m-%d %H:%M:%S")
        print(f"ğŸ• PER/EPS í¬ë¡¤ë§ ì‹œì‘: {timestamp}")
        
    def end_timer(self):
        """í¬ë¡¤ë§ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° ì†Œìš”ì‹œê°„ ê³„ì‚°"""
        self.end_time = datetime.now()
        timestamp = self.end_time.strftime("%Y-%m-%d %H:%M:%S")
        print(f"ğŸ• PER/EPS í¬ë¡¤ë§ ì¢…ë£Œ: {timestamp}")
        
        if self.start_time:
            duration = self.end_time - self.start_time
            total_seconds = int(duration.total_seconds())
            hours = total_seconds // 3600
            minutes = (total_seconds % 3600) // 60
            seconds = total_seconds % 60
            
            if hours > 0:
                print(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {hours}ì‹œê°„ {minutes}ë¶„ {seconds}ì´ˆ")
            elif minutes > 0:
                print(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {minutes}ë¶„ {seconds}ì´ˆ")
            else:
                print(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {seconds}ì´ˆ")
    
    async def initialize_browser(self):
        """ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        try:
            print("â–¶ [1/4] Playwright_async starting...") # ë¡œê·¸ ì¶”ê°€
            self.playwright = await async_playwright().start()
            print("â–¶ [2/4] Playwright_async started successfully.") # ë¡œê·¸ ì¶”ê°€

            # --- ìˆ˜ì •ëœ ë¶€ë¶„: self.browser_args ì‚¬ìš© ---
            print(f"â–¶ [3/4] Launching browser with args: {self.browser_args}")
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless,
                args=self.browser_args # í•˜ë“œì½”ë”©ëœ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ë³€ìˆ˜ ì‚¬ìš©
            )
            print("â–¶ [4.1/5] Browser object created.") # ë¡œê·¸ ì¶”ê°€

            print("â–¶ [4.2/5] Creating new page...")
            self.page = await self.browser.new_page()
            print("â–¶ [5/5] New page created successfully.")
            # --- ì—¬ê¸°ê¹Œì§€ ---

            # User-Agent ì„¤ì • ë“± ë‚˜ë¨¸ì§€ ì½”ë“œ...
            await self.page.set_extra_http_headers({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            print("âœ… ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
            return True

        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            print(traceback.format_exc()) # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì „ì²´ ì¶œë ¥
            return False
    
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            if self.page:
                await self.page.close()
            if self.browser:
                await self.browser.close()
            if hasattr(self, 'playwright'):
                await self.playwright.stop()
            print("âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def crawl_per_eps_data(self, stock_code="004150", company_name="í•œì†”í™€ë”©ìŠ¤"):
        """
        ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ PER/EPS ì •ë³´ í¬ë¡¤ë§
        
        Args:
            stock_code (str): ì£¼ì‹ì½”ë“œ (ì˜ˆ: "004150")
            company_name (str): íšŒì‚¬ëª…
            
        Returns:
            dict: í¬ë¡¤ë§ëœ PER/EPS ë°ì´í„°
        """
        url = f"https://finance.naver.com/item/main.naver?code={stock_code}"
        
        try:
            print(f"ğŸ“Š {company_name}({stock_code}) PER/EPS ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘...")
            print(f"ğŸ”— URL: {url}")
            
            # í˜ì´ì§€ ì´ë™
            await self.page.goto(url, wait_until="networkidle")
            await asyncio.sleep(1)
            
            # ë™ì  ë°ì´í„° ë¡œë”©ì„ ìœ„í•œ ì¶”ê°€ ëŒ€ê¸°
            print("â³ ë™ì  ë°ì´í„° ë¡œë”© ëŒ€ê¸° ì¤‘...")
            await asyncio.sleep(1)
            
            # ë™ì  PBR/PER ê°’ë“¤ì„ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            print("ğŸ” ë™ì  PBR/PER ê°’ í™•ì¸ ì¤‘...")
            dynamic_values = {}
            try:
                # íŠ¹ì • IDë¡œ ë™ì  ê°’ë“¤ ê°€ì ¸ì˜¤ê¸°
                target_ids = ['_per', '_pbr', '_eps', '_bps']
                for target_id in target_ids:
                    try:
                        element = await self.page.query_selector(f'#{target_id}')
                        if element:
                            text = await element.inner_text()
                            dynamic_values[target_id] = text.strip()
                            print(f"ğŸ¯ {target_id}: {text.strip()}")
                    except Exception as e:
                        print(f"âš ï¸ {target_id} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                
                print(f"âœ… ë™ì  ê°’ ìˆ˜ì§‘ ì™„ë£Œ: {dynamic_values}")
                
            except Exception as e:
                print(f"âš ï¸ ë™ì  ìš”ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            # í˜ì´ì§€ êµ¬ì¡° ë””ë²„ê¹…
            print("ğŸ” í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ì¤‘...")
            
            # aside_invest_info div ì°¾ê¸°
            print("ğŸ” íˆ¬ìì •ë³´ ì˜ì—­ ì°¾ëŠ” ì¤‘...")
            aside_invest_info = await self.page.query_selector('#aside_invest_info')
            
            if not aside_invest_info:
                print("âŒ aside_invest_infoë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # PER í‚¤ì›Œë“œê°€ í¬í•¨ëœ í…Œì´ë¸”ë“¤ ì§ì ‘ ì°¾ê¸°
                print("ğŸ” PER í‚¤ì›Œë“œê°€ í¬í•¨ëœ í…Œì´ë¸”ë“¤ ì°¾ëŠ” ì¤‘...")
                all_tables = await self.page.query_selector_all('table')
                per_tables = []
                for i, table in enumerate(all_tables):
                    summary = await table.get_attribute('summary')
                    if summary and ('PER' in summary or 'EPS' in summary):
                        per_tables.append((i, table, summary))
                        print(f"   í…Œì´ë¸” {i}: summary='{summary}'")
                
                if per_tables:
                    print(f"âœ… PER/EPS ê´€ë ¨ í…Œì´ë¸” {len(per_tables)}ê°œ ë°œê²¬!")
                    # ì²« ë²ˆì§¸ PER/EPS í…Œì´ë¸” ì‚¬ìš©
                    _, selected_table, selected_summary = per_tables[0]
                    print(f"ğŸ¯ ì„ íƒëœ í…Œì´ë¸”: {selected_summary}")
                    return await self.extract_table_data(selected_table, stock_code, company_name, dynamic_values)
                else:
                    print("âŒ PER/EPS ê´€ë ¨ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
            
            print("âœ… íˆ¬ìì •ë³´ ì˜ì—­ ë°œê²¬")
            
            # PER/EPS ì •ë³´ í…Œì´ë¸” ì°¾ê¸°
            print("ğŸ” PER/EPS ì •ë³´ í…Œì´ë¸” ì°¾ëŠ” ì¤‘...")
            per_eps_table = await aside_invest_info.query_selector('table[summary="PER/EPS ì •ë³´"]')
            
            if not per_eps_table:
                print("âŒ PER/EPS ì •ë³´ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # ëª¨ë“  í…Œì´ë¸” í™•ì¸í•´ë³´ê¸°
                print("ğŸ” íˆ¬ìì •ë³´ ì˜ì—­ ë‚´ ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”ë“¤:")
                tables = await aside_invest_info.query_selector_all('table')
                for i, table in enumerate(tables):
                    summary = await table.get_attribute('summary')
                    print(f"   í…Œì´ë¸” {i+1}: summary='{summary}'")
                return None
            
            print("âœ… PER/EPS ì •ë³´ í…Œì´ë¸” ë°œê²¬")
            
            # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
            return await self.extract_table_data(per_eps_table, stock_code, company_name, dynamic_values)
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
    
    async def extract_table_data(self, table, stock_code, company_name, dynamic_values=None):
        """
        í…Œì´ë¸”ì—ì„œ PER/EPS ë°ì´í„° ì¶”ì¶œ
        
        Args:
            table: Playwright í…Œì´ë¸” ìš”ì†Œ
            stock_code (str): ì£¼ì‹ì½”ë“œ
            company_name (str): íšŒì‚¬ëª…
            dynamic_values (dict): ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¨ PER/PBR ê°’ë“¤
            
        Returns:
            dict: ì¶”ì¶œëœ ë°ì´í„°
        """
        try:
            print("ğŸ“‹ í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì¤‘...")
            
            # í…Œì´ë¸”ì˜ ëª¨ë“  í–‰ ê°€ì ¸ì˜¤ê¸°
            rows = await table.query_selector_all('tr')
            
            data = {
                'stock_code': stock_code,
                'company_name': company_name,
                'crawl_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'per_eps_data': []
            }
            
            for row_idx, row in enumerate(rows):
                # ê° í–‰ì˜ ì…€ë“¤ ê°€ì ¸ì˜¤ê¸°
                cells = await row.query_selector_all('td, th')
                
                if len(cells) >= 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì…€ì´ ìˆì–´ì•¼ ì˜ë¯¸ìˆëŠ” ë°ì´í„°
                    row_data = []
                    for cell in cells:
                        cell_text = await cell.inner_text()
                        cell_text = cell_text.strip()
                        row_data.append(cell_text)
                    
                    if row_data and any(text for text in row_data):  # ë¹ˆ í–‰ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                        data['per_eps_data'].append({
                            'row_index': row_idx,
                            'cells': row_data
                        })
                        print(f"   í–‰ {row_idx}: {' | '.join(row_data)}")
            
            print(f"âœ… ì´ {len(data['per_eps_data'])}ê°œ í–‰ì˜ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            
            return data
            
        except Exception as e:
            print(f"âŒ í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def parse_per_eps_data(self, data):
        """
        PER/EPS ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ íŒŒì‹±
        ê° í•­ëª©ì„ ê°œë³„ í–‰ìœ¼ë¡œ ë¶„ë¦¬ (PER|EPS -> PER í–‰, EPS í–‰)
        
        Args:
            data (dict): ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„°
            
        Returns:
            list: íŒŒì‹±ëœ êµ¬ì¡°í™”ëœ ë°ì´í„°
        """
        parsed_data = []
        
        for row_data in data['per_eps_data']:
            if len(row_data['cells']) >= 2:
                item_name = row_data['cells'][0].strip()
                item_value = row_data['cells'][1].strip()
                
                # "l"ë¡œ êµ¬ë¶„ëœ ê°’ë“¤ ë¶„ë¦¬
                if 'l' in item_value:
                    value_parts = item_value.split('l')
                    left_value = value_parts[0].strip()
                    right_value = value_parts[1].strip() if len(value_parts) > 1 else ""
                else:
                    left_value = item_value
                    right_value = ""
                
                # í•­ëª©ëª…ì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                date_match = re.search(r'\((\d{4}\.\d{2})\)', item_name)
                date_info = date_match.group(1) if date_match else ""
                
                # í•­ëª©ëª… ì •ë¦¬ (ë‚ ì§œ ì •ë³´ ì œê±°)
                clean_item_name = re.sub(r'\(\d{4}\.\d{2}\)', '', item_name).strip()
                
                # "l"ë¡œ êµ¬ë¶„ëœ í•­ëª©ëª… ì²˜ë¦¬í•˜ì—¬ ê°ê°ì„ ê°œë³„ í–‰ìœ¼ë¡œ ë¶„ë¦¬
                if 'l' in clean_item_name:
                    name_parts = clean_item_name.split('l')
                    main_item = name_parts[0].strip()
                    sub_item = name_parts[1].strip() if len(name_parts) > 1 else ""
                    
                    # ë°°ë‹¹ìˆ˜ìµë¥ ì˜ ê²½ìš° ë‚ ì§œ í•„ë“œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
                    if main_item == "ë°°ë‹¹ìˆ˜ìµë¥ ":
                        # ë‚ ì§œ ì •ë³´ë¥¼ í•­ëª©ëª…ì—ì„œ ì¶”ì¶œí•˜ë˜, ë³„ë„ í–‰ìœ¼ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ
                        date_match_from_sub = re.search(r'\d{4}\.\d{2}', sub_item)
                        extracted_date = date_match_from_sub.group() if date_match_from_sub else date_info
                        
                        parsed_data.append({
                            'stock_code': data['stock_code'],
                            'company_name': data['company_name'],
                            'crawl_time': data['crawl_time'],
                            'item_type': main_item,
                            'item_category': 'main',
                            'date_info': extracted_date,
                            'value': left_value,
                            'unit': self._extract_unit(left_value),
                            'numeric_value': self._extract_numeric_value(left_value),
                            'raw_item_name': item_name,
                            'raw_item_value': item_value,
                            'original_row_index': row_data['row_index']
                        })
                    else:
                        # ê¸°ì¡´ ë¡œì§: PER/EPS ë“±ì€ ë‘ ê°œì˜ í–‰ìœ¼ë¡œ ë¶„ë¦¬
                        # ì²« ë²ˆì§¸ í•­ëª© (ì˜ˆ: PER, ì¶”ì •PER)
                        parsed_data.append({
                            'stock_code': data['stock_code'],
                            'company_name': data['company_name'],
                            'crawl_time': data['crawl_time'],
                            'item_type': main_item,
                            'item_category': 'main',
                            'date_info': date_info,
                            'value': left_value,
                            'unit': self._extract_unit(left_value),
                            'numeric_value': self._extract_numeric_value(left_value),
                            'raw_item_name': item_name,
                            'raw_item_value': item_value,
                            'original_row_index': row_data['row_index']
                        })
                        
                        # ë‘ ë²ˆì§¸ í•­ëª© (ì˜ˆ: EPS, ì¶”ì •EPS)
                        if sub_item:
                            # "ì¶”ì •PER"ì˜ ê²½ìš° sub_itemì„ "ì¶”ì •EPS"ë¡œ ë³€ê²½
                            if main_item == "ì¶”ì •PER" and sub_item == "EPS":
                                sub_item = "ì¶”ì •EPS"
                            
                            parsed_data.append({
                                'stock_code': data['stock_code'],
                                'company_name': data['company_name'],
                                'crawl_time': data['crawl_time'],
                                'item_type': sub_item,
                                'item_category': 'sub',
                                'date_info': date_info,
                                'value': right_value,
                                'unit': self._extract_unit(right_value),
                                'numeric_value': self._extract_numeric_value(right_value),
                                'raw_item_name': item_name,
                                'raw_item_value': item_value,
                                'original_row_index': row_data['row_index']
                            })
                else:
                    # "l"ë¡œ êµ¬ë¶„ë˜ì§€ ì•Šì€ ë‹¨ì¼ í•­ëª©
                    parsed_data.append({
                        'stock_code': data['stock_code'],
                        'company_name': data['company_name'],
                        'crawl_time': data['crawl_time'],
                        'item_type': clean_item_name,
                        'item_category': 'single',
                        'date_info': date_info,
                        'value': left_value,
                        'unit': self._extract_unit(left_value),
                        'numeric_value': self._extract_numeric_value(left_value),
                        'raw_item_name': item_name,
                        'raw_item_value': item_value,
                        'original_row_index': row_data['row_index']
                    })
        
        return parsed_data
    
    def _extract_unit(self, value_str):
        """ê°’ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ (ë°°, ì›, % ë“±)"""
        if not value_str or value_str == 'N/A':
            return ""
        
        # ì¼ë°˜ì ì¸ ë‹¨ìœ„ë“¤ ì°¾ê¸°
        units = ['ë°°', 'ì›', '%', 'ì–µì›', 'ë§Œì›']
        for unit in units:
            if unit in value_str:
                return unit
        return ""
    
    def _extract_numeric_value(self, value_str):
        """ê°’ì—ì„œ ìˆ«ì ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
        if not value_str or value_str == 'N/A':
            return None
        
        # ìˆ«ìì™€ ì†Œìˆ˜ì , ì½¤ë§ˆë§Œ ì¶”ì¶œ
        numeric_match = re.search(r'[\d,]+\.?\d*', value_str.replace(',', ''))
        if numeric_match:
            try:
                return float(numeric_match.group().replace(',', ''))
            except ValueError:
                return None
        return None


def convert_results_to_csv(batch_results):
    """
    ë°°ì¹˜ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ CSV í˜•íƒœë¡œ ë³€í™˜ (UTF-8 BOM í¬í•¨)
    
    Args:
        batch_results (dict): ë°°ì¹˜ í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„°
        
    Returns:
        str: CSV í˜•íƒœì˜ ë¬¸ìì—´ (UTF-8 BOM í¬í•¨)
    """
    if not batch_results.get("success") or not batch_results.get("results"):
        return ""
    
    csv_buffer = io.StringIO()
    csv_writer = csv.writer(csv_buffer)
    
    # CSV í—¤ë” ì‘ì„±
    headers = [
        'stock_code', 'company_name', 'crawl_time', 'item_type', 
        'item_category', 'date_info', 'raw_value', 'unit', 'value',
        'raw_item_name', 'raw_item_value', 'original_row_index'
    ]
    csv_writer.writerow(headers)
    
    # ê° ì¢…ëª©ì˜ íŒŒì‹±ëœ ë°ì´í„°ë¥¼ CSV í–‰ìœ¼ë¡œ ë³€í™˜
    for result in batch_results["results"]:
        if result.get("success") and result.get("parsed_data"):
            for item in result["parsed_data"]:
                row = [
                    str(item.get('stock_code', '')),
                    str(item.get('company_name', '')),
                    str(item.get('crawl_time', '')),
                    str(item.get('item_type', '')),
                    str(item.get('item_category', '')),
                    str(item.get('date_info', '')),
                    str(item.get('value', '')),  # raw_value ì»¬ëŸ¼
                    str(item.get('unit', '')),
                    item.get('numeric_value', ''),  # value ì»¬ëŸ¼ (ìˆ«ì)
                    str(item.get('raw_item_name', '')),
                    str(item.get('raw_item_value', '')),
                    str(item.get('original_row_index', ''))
                ]
                csv_writer.writerow(row)
    
    csv_content = csv_buffer.getvalue()
    csv_buffer.close()
    
    # UTF-8 BOM ì¶”ê°€ (í•œê¸€ ê¹¨ì§ ë°©ì§€)
    return '\ufeff' + csv_content




async def run_batch_per_eps_crawler_for_lambda(stocks, headless=True, delay_between_stocks=2):
    """
    Lambdaìš© ë°°ì¹˜ PER/EPS í¬ë¡¤ëŸ¬ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        stocks (list): ì¢…ëª© ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{"code": "004150", "name": "í•œì†”í™€ë”©ìŠ¤"}, ...]
        headless (bool): í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
        delay_between_stocks (int): ì¢…ëª© ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        
    Returns:
        dict: ë°°ì¹˜ í¬ë¡¤ë§ ê²°ê³¼ JSON ë°ì´í„°
    """
    crawler = NaverFinancePERCrawlerForLambda(headless=headless)
    crawler.start_timer()
    
    all_results = []
    successful_count = 0
    failed_count = 0
    
    try:
        # ë¸Œë¼ìš°ì € ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ì—¬ ì¬ì‚¬ìš©)
        if not await crawler.initialize_browser():
            return {"success": False, "error": "ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨"}
        
        print(f"ğŸš€ ì´ {len(stocks)}ê°œ ì¢…ëª© ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 60)
        
        for idx, stock in enumerate(stocks, 1):
            stock_code = stock.get('code', '')
            company_name = stock.get('name', '')
            
            print(f"[{idx}/{len(stocks)}] {company_name}({stock_code}) í¬ë¡¤ë§ ì¤‘...")
            
            try:
                # ê°œë³„ ì¢…ëª© í¬ë¡¤ë§
                data = await crawler.crawl_per_eps_data(stock_code, company_name)
                
                if data:
                    parsed_data = crawler.parse_per_eps_data(data)
                    all_results.append({
                        "stock_code": stock_code,
                        "company_name": company_name,
                        "success": True,
                        "raw_data": data,
                        "parsed_data": parsed_data
                    })
                    successful_count += 1
                    print(f"âœ… {company_name}({stock_code}) í¬ë¡¤ë§ ì„±ê³µ")
                else:
                    all_results.append({
                        "stock_code": stock_code,
                        "company_name": company_name,
                        "success": False,
                        "error": "ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                    })
                    failed_count += 1
                    print(f"âŒ {company_name}({stock_code}) í¬ë¡¤ë§ ì‹¤íŒ¨")
                
            except Exception as e:
                all_results.append({
                    "stock_code": stock_code,
                    "company_name": company_name,
                    "success": False,
                    "error": str(e)
                })
                failed_count += 1
                print(f"âŒ {company_name}({stock_code}) í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            # ì¢…ëª© ê°„ ë”œë ˆì´ (ë§ˆì§€ë§‰ ì¢…ëª©ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
            if idx < len(stocks) and delay_between_stocks > 0:
                print(f"â³ {delay_between_stocks}ì´ˆ ëŒ€ê¸° ì¤‘...")
                await asyncio.sleep(delay_between_stocks)
        
        # ìµœì¢… ê²°ê³¼
        result = {
            "success": True,
            "batch_info": {
                "total_stocks": len(stocks),
                "successful_count": successful_count,
                "failed_count": failed_count,
                "success_rate": f"{(successful_count/len(stocks)*100):.1f}%",
                "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            },
            "results": all_results
        }
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ì¢…ëª© ìˆ˜: {len(stocks)}")
        print(f"âœ… ì„±ê³µ: {successful_count}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {(successful_count/len(stocks)*100):.1f}%")
        print("=" * 60)
        print("ğŸ“Š ë°°ì¹˜ í¬ë¡¤ë§ ê²°ê³¼ JSON:")
        print(json.dumps(result, ensure_ascii=False, indent=2))
        print("=" * 60)
        
        return result
        
    except Exception as e:
        return {"success": False, "error": f"ë°°ì¹˜ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}"}
    
    finally:
        await crawler.close_browser()
        crawler.end_timer()


# Lambda í•¸ë“¤ëŸ¬ í•¨ìˆ˜
def lambda_handler(event, context):
    """
    AWS Lambda í•¸ë“¤ëŸ¬ í•¨ìˆ˜ - stocks.json íŒŒì¼ ê¸°ë°˜ ë°°ì¹˜ í¬ë¡¤ë§
    
    Args:
        event (dict): Lambda ì´ë²¤íŠ¸ ë°ì´í„°
        context: Lambda ì»¨í…ìŠ¤íŠ¸
        
    Returns:
        dict: HTTP ì‘ë‹µ
    """
    try:
        # stocks.json íŒŒì¼ì—ì„œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        try:
            with open('stocks.json', 'r', encoding='utf-8') as f:
                stocks = json.load(f)
            print(f"ğŸ“‹ stocks.jsonì—ì„œ {len(stocks)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json; charset=utf-8'
                },
                'body': json.dumps({
                    'success': False,
                    'error': f'stocks.json íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}'
                }, ensure_ascii=False, indent=2)
            }
        
        # ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
        print("ğŸš€ stocks.json ê¸°ë°˜ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘")
        delay_between_stocks = event.get('delay_between_stocks', 2)
        
        result = asyncio.run(run_batch_per_eps_crawler_for_lambda(
            stocks=stocks,
            headless=True,
            delay_between_stocks=delay_between_stocks
        ))
        
        # S3 ì—…ë¡œë“œ ì²˜ë¦¬
        s3_upload_result = None
        if result.get("success"):
            try:
                # S3 ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ìš°ì„ ìˆœìœ„: í™˜ê²½ë³€ìˆ˜ > ì´ë²¤íŠ¸ > ê¸°ë³¸ê°’)
                bucket_name = os.environ.get('S3_BUCKET') or event.get('s3_bucket') or 'test-stock-info-bucket'
                
                # ë™ì  S3 í‚¤ ìƒì„± (ëŒë‹¤ ì‹¤í–‰ ì‹œê°„ ê¸°ì¤€)
                current_time = datetime.now()
                s3_key = generate_s3_key("daily", current_time)
                
                print(f"ğŸ“¤ S3 ì—…ë¡œë“œ ì¤€ë¹„: s3://{bucket_name}/{s3_key}")
                
                # í¬ë¡¤ë§ ê²°ê³¼ë¥¼ CSVë¡œ ë³€í™˜
                csv_content = convert_results_to_csv(result)
                
                if csv_content:
                    # S3ì— CSV ì—…ë¡œë“œ
                    s3_upload_result = upload_csv_content_to_s3(csv_content, bucket_name, s3_key)
                    
                    if s3_upload_result.get("success"):
                        print(f"âœ… S3 ì—…ë¡œë“œ ì„±ê³µ: {s3_upload_result['s3_url']}")
                        
                        # ì‘ë‹µì— S3 ì •ë³´ ì¶”ê°€
                        result["s3_upload"] = s3_upload_result
                    else:
                        print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_upload_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                        result["s3_upload"] = {
                            "success": False,
                            "error": s3_upload_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                        }
                else:
                    print("âš ï¸ CSV ë³€í™˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆì–´ S3 ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                    result["s3_upload"] = {
                        "success": False,
                        "error": "CSV ë³€í™˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ"
                    }
                    
            except Exception as e:
                print(f"âŒ S3 ì—…ë¡œë“œ ê³¼ì •ì—ì„œ ì˜¤ë¥˜: {str(e)}")
                result["s3_upload"] = {
                    "success": False,
                    "error": f"S3 ì—…ë¡œë“œ ê³¼ì •ì—ì„œ ì˜¤ë¥˜: {str(e)}"
                }
        
        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json; charset=utf-8'
            },
            'body': json.dumps(result, ensure_ascii=False, indent=2)
        }
        
    except Exception as e:
        return {
            'statusCode': 500,
            'headers': {
                'Content-Type': 'application/json; charset=utf-8'
            },
            'body': json.dumps({
                'success': False,
                'error': f'Lambda ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }, ensure_ascii=False, indent=2)
        }


if __name__ == "__main__":
    print("ğŸš€ ë„¤ì´ë²„ ê¸ˆìœµ íˆ¬ìì •ë³´ í¬ë¡¤ëŸ¬ (Lambdaìš©) - stocks.json ë°°ì¹˜ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # stocks.json íŒŒì¼ ê¸°ë°˜ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
    try:
        with open('stocks.json', 'r', encoding='utf-8') as f:
            stocks = json.load(f)
        print(f"ğŸ“‹ stocks.jsonì—ì„œ {len(stocks)}ê°œ ì¢…ëª© ë¡œë“œ")
        
        print("ğŸ“Š ë°°ì¹˜ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        result = asyncio.run(run_batch_per_eps_crawler_for_lambda(
            stocks=stocks,
            headless=True,
            delay_between_stocks=2
        ))
        
        if result.get("success"):
            print("âœ… Lambdaìš© ë°°ì¹˜ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            print(f"ğŸ“ˆ ì„±ê³µë¥ : {result['batch_info']['success_rate']}")
            
            # S3 ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸ (ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ê±´ë„ˆë›°ê¸°)
            print("\nğŸ“¤ S3 ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸...")
            try:
                # ë™ì  S3 í‚¤ ìƒì„±
                current_time = datetime.now()
                s3_key = generate_s3_key("daily", current_time)
                bucket_name = 'test-stock-info-bucket'
                
                print(f"ğŸ“ ì˜ˆìƒ S3 ê²½ë¡œ: s3://{bucket_name}/{s3_key}")
                
                # CSV ë³€í™˜ í…ŒìŠ¤íŠ¸
                csv_content = convert_results_to_csv(result)
                if csv_content:
                    print(f"âœ… CSV ë³€í™˜ ì„±ê³µ (í¬ê¸°: {len(csv_content)} bytes)")
                    print("ğŸ“„ CSV ì²« 10ì¤„ ë¯¸ë¦¬ë³´ê¸°:")
                    lines = csv_content.split('\n')[:10]
                    for i, line in enumerate(lines, 1):
                        print(f"   {i}: {line}")
                else:
                    print("âŒ CSV ë³€í™˜ ì‹¤íŒ¨")
                
                # S3 ì—…ë¡œë“œ ì‹œë„ (AWS CLI ì„¤ì •ì´ ìˆëŠ” ê²½ìš°)
                print("ğŸ“¤ S3 ì—…ë¡œë“œ ì‹œë„ ì¤‘...")
                try:
                    s3_upload_result = upload_csv_content_to_s3(csv_content, bucket_name, s3_key)
                    
                    if s3_upload_result.get("success"):
                        print(f"âœ… S3 ì—…ë¡œë“œ ì„±ê³µ!")
                        print(f"ğŸ“ ì—…ë¡œë“œëœ ìœ„ì¹˜: {s3_upload_result['s3_url']}")
                        print(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {s3_upload_result['size']} bytes")
                    else:
                        print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_upload_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                        print("ğŸ’¡ AWS CLIê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€, S3 ë²„í‚·ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        
                except Exception as upload_error:
                    print(f"âŒ S3 ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(upload_error)}")
                    print("ğŸ’¡ AWS CLI ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”: aws configure list")
                
            except Exception as e:
                print(f"âŒ S3 ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        else:
            print("âŒ Lambdaìš© ë°°ì¹˜ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
            print(f"ğŸš¨ ì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            
    except FileNotFoundError:
        print("âŒ stocks.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")